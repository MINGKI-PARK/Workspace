{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "324810e6-49a0-4eb4-8da1-0079e7b60c80",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 텍스트 전처리\n",
    "+ 토큰(글자, 단어, 문장, 문단)화\n",
    "+ 불용어\n",
    "+ 정규표현식\n",
    "+ 인코딩(단어를 숫자로 변환), 패딩(길이를 동일하게)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22e4aca1-04f5-4105-abce-ca9546536971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt, Kkma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ac6079f-e49c-4b0b-8f24-ebeb03d439dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['단독', '입찰', '보다', '복수', '입찰', '의', '경우']\n"
     ]
    }
   ],
   "source": [
    "okt = Okt()\n",
    "print(okt.morphs(u'단독입찰보다 복수입찰의 경우'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f6110e-8c54-48a7-9650-5a2d1a34a48d",
   "metadata": {},
   "source": [
    "+ 단어 가방(Bag of words: BOW): 안어들이 순서와 관계없이 저장, 사전\n",
    "+ 코퍼스(corpus: 말뭉치): 자연어 처리(분석)하고자 하는 분야와 관련된 단어 집합\n",
    "    - ex) 법률 서비스 챗봇 -> 법률 코퍼스: 기소, 벌금...\n",
    "    - ex) 관광 서비스 챗봇 -> 관광 코퍼스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2857ddb-063e-4b56-80f4-eb9b73e999a3",
   "metadata": {},
   "source": [
    "+ nltk: 영어 코퍼스 토큰화 도구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "512ebb17-e8ec-4210-ab03-16facb0ac4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Playdata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Playdata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9ef7210-599c-47bf-adea-077d287fba6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, WordPunctTokenizer, sent_tokenize\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fa060c9-587d-4c94-a108-1fc88dcc1c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Language is a thing of beauty.',\n",
       " 'But mastering a new language from scratch is quite a daunting prospect.',\n",
       " 'If you’ve ever picked up a language that wasn’t your mother tongue, you’ll relate to this!',\n",
       " 'There are so many layers to peel off and syntaxes to consider – it’s quite a challenge.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = 'Language is a thing of beauty. But mastering a new language from scratch is quite a daunting prospect. If you’ve ever picked up a language that wasn’t your mother tongue, you’ll relate to this! There are so many layers to peel off and syntaxes to consider – it’s quite a challenge.'\n",
    "\n",
    "sent_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "917c0e2c-3325-4cb5-989b-2c6f155d0b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화 결과 : ['Tommy', '’', 's', 'Don', '’', 't', 'And', 'that', '’', 's', 'exactly', 'the', 'way', 'with', 'our', 'machines', '.', 'In', 'order', 'to', 'get', 'our', 'computer', 'to', 'understand', 'any', 'text', ',', 'we', 'need', 'to', 'break', 'that', 'word', 'down', 'in', 'a', 'way', 'that', 'our', 'machine', 'can', 'understand', '.', 'That', '’', 's', 'where', 'the', 'concept', 'of', 'tokenization', 'in', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'comes', 'in', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Tommy', 'NNP'),\n",
       " ('’', 'NNP'),\n",
       " ('s', 'VBD'),\n",
       " ('Don', 'NNP'),\n",
       " ('’', 'NNP'),\n",
       " ('t', 'NN'),\n",
       " ('And', 'CC'),\n",
       " ('that', 'IN'),\n",
       " ('’', 'NNP'),\n",
       " ('s', 'VBZ'),\n",
       " ('exactly', 'RB'),\n",
       " ('the', 'DT'),\n",
       " ('way', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('our', 'PRP$'),\n",
       " ('machines', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('In', 'IN'),\n",
       " ('order', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('get', 'VB'),\n",
       " ('our', 'PRP$'),\n",
       " ('computer', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('understand', 'VB'),\n",
       " ('any', 'DT'),\n",
       " ('text', 'NN'),\n",
       " (',', ','),\n",
       " ('we', 'PRP'),\n",
       " ('need', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('break', 'VB'),\n",
       " ('that', 'IN'),\n",
       " ('word', 'NN'),\n",
       " ('down', 'RP'),\n",
       " ('in', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('way', 'NN'),\n",
       " ('that', 'IN'),\n",
       " ('our', 'PRP$'),\n",
       " ('machine', 'NN'),\n",
       " ('can', 'MD'),\n",
       " ('understand', 'VB'),\n",
       " ('.', '.'),\n",
       " ('That', 'DT'),\n",
       " ('’', 'VBZ'),\n",
       " ('s', 'NN'),\n",
       " ('where', 'WRB'),\n",
       " ('the', 'DT'),\n",
       " ('concept', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('tokenization', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('Natural', 'NNP'),\n",
       " ('Language', 'NNP'),\n",
       " ('Processing', 'NNP'),\n",
       " ('(', '('),\n",
       " ('NLP', 'NNP'),\n",
       " (')', ')'),\n",
       " ('comes', 'VBZ'),\n",
       " ('in', 'IN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('단어 토큰화 결과 :', word_tokenize('Tommy’s Don’t And that’s exactly the way with our machines. In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand. That’s where the concept of tokenization in Natural Language Processing (NLP) comes in.'))\n",
    "\n",
    "res = word_tokenize('Tommy’s Don’t And that’s exactly the way with our machines. In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand. That’s where the concept of tokenization in Natural Language Processing (NLP) comes in.')\n",
    "pos_tag(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6e4c97-e750-4cda-ac0f-a84138b19e13",
   "metadata": {},
   "source": [
    "+ 품사\n",
    "    - https://happygrammer.github.io/nlp/postag-set/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd1d2c21-f6ef-42ae-8be3-54fefd1f66fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 문장 토큰화 도구\n",
    "import kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be399a7b-93dc-444f-9d47-25f17cacb791",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Kss]: Because there's no supported C++ morpheme analyzer, Kss will take pecab as a backend. :D\n",
      "For your information, Kss also supports mecab backend.\n",
      "We recommend you to install mecab or konlpy.tag.Mecab for faster execution of Kss.\n",
      "Please refer to following web sites for details:\n",
      "- mecab: https://cleancode-ws.tistory.com/97\n",
      "- konlpy.tag.Mecab: https://uwgdqo.tistory.com/363\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['여름입니다.', '날씨가 덥습니다!', '딥러닝을 공부합니다.', '네?']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '여름입니다. 날씨가 덥습니다! 딥러닝을 공부합니다. 네?'\n",
    "\n",
    "kss.split_sentences(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf5522b2-3724-4346-b12f-d672fad1e67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kkma = Kkma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7efd38f-fcc2-491a-a891-268c906d13c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okt:  ['NLP', '를', '열심히', '공부', '하고', ',', '취업', '에', '성공합시다']\n",
      "Okt:  [('NLP', 'Alpha'), ('를', 'Noun'), ('열심히', 'Adverb'), ('공부', 'Noun'), ('하고', 'Josa'), (',', 'Punctuation'), ('취업', 'Noun'), ('에', 'Josa'), ('성공합시다', 'Adjective')]\n",
      "Okt:  ['를', '공부', '취업']\n"
     ]
    }
   ],
   "source": [
    "print('Okt: ', okt.morphs('NLP를 열심히 공부하고, 취업에 성공합시다'))\n",
    "print('Okt: ', okt.pos('NLP를 열심히 공부하고, 취업에 성공합시다'))\n",
    "print('Okt: ', okt.nouns('NLP를 열심히 공부하고, 취업에 성공합시다'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65fbc65f-2937-4626-bb65-b696ea7079fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kkma:  ['NLP', '를', '열심히', '공부', '하', '고', ',', '취업', '에', '성공', '합', '시다']\n",
      "Kkma:  [('NLP', 'OL'), ('를', 'JKO'), ('열심히', 'MAG'), ('공부', 'NNG'), ('하', 'XSV'), ('고', 'ECE'), (',', 'SP'), ('취업', 'NNG'), ('에', 'JKM'), ('성공', 'NNG'), ('합', 'NNG'), ('시다', 'NNG')]\n",
      "Kkma:  ['공부', '취업', '성공', '성공합시다', '합', '시다']\n"
     ]
    }
   ],
   "source": [
    "print('Kkma: ', kkma.morphs('NLP를 열심히 공부하고, 취업에 성공합시다'))\n",
    "print('Kkma: ', kkma.pos('NLP를 열심히 공부하고, 취업에 성공합시다'))\n",
    "print('Kkma: ', kkma.nouns('NLP를 열심히 공부하고, 취업에 성공합시다'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e33c40-3cfd-4bde-ad3f-d4b0556d3c5e",
   "metadata": {},
   "source": [
    "+ 빈도수가 낮거나 길이가 짧은 단어는 상황에 따라 제거 고려\n",
    "    - 저게하는 것이 성능에 더 도움이 된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ba55e4f-6351-4f86-9bcf-fa112db5c72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규표현식\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da96300a-1747-4e95-8930-3bcfcc62ed3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tommy Don And that exactly the way with our machines order get our computer understand any text need break that word down way that our machine can understand. That where the concept tokenization Natural Language Processing (NLP) comes.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Tommy’s Don’t And that’s exactly the way with our machines. In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand. That’s where the concept of tokenization in Natural Language Processing (NLP) comes in.'\n",
    "\n",
    "pat = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "pat.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c1c5e343-0fff-499f-82fa-ff7a2bbc7734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dbb4be67-37c7-4a3d-a3a1-0ff71f22a611",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a9cb6815-977f-4a71-9862-25070bddb8c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw_eng = set(stopwords.words('english'))\n",
    "len(sw_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d2f27b7d-b8a4-47b1-b50f-4e623434de6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(text)\n",
    "res = []\n",
    "\n",
    "for word in words:\n",
    "    if word not in sw_eng:\n",
    "        res.append(word)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "print(len(words))\n",
    "print(len(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a2643a-1432-473d-b112-4037fc83710c",
   "metadata": {},
   "source": [
    "+ NLTK's list of english stopwords\n",
    "    - https://gist.github.com/sebleier/554280"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8798f95-3f9a-4d36-8f59-6e3d2c754b05",
   "metadata": {},
   "source": [
    "### 한글 stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "75f6406f-db1c-4c1c-9203-120780d56e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['을', '를', '에', '고', '라고', '다', '하고']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'NLP를 열심히 공부하고, 취업에 성공합시다'\n",
    "\n",
    "sw_kor = '을 를 에 고 라고 다 하고'\n",
    "stopwords_kor = sw_kor.split(' ')\n",
    "stopwords_kor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2751aa51-6a6c-411f-b90d-dc5db4f3550a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLP', '열심히', '공부', ',', '취업', '성공합시다']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = okt.morphs(text)\n",
    "res = []\n",
    "\n",
    "[word for word in words if word not in stopwords_kor]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b45b04-cbb9-4ced-874c-e3402d561975",
   "metadata": {},
   "source": [
    "+ 한국어 불용어 리스트(완벽하지는 않다)\n",
    "    - https://deep.chulgil.me/hangugeo-bulyongeo-riseuteu/\n",
    "    - https://github.com/stopwords-iso/stopwords-ko/blob/master/stopwords-ko.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32c8b1e-3b0c-4313-8fc0-499a1175652a",
   "metadata": {},
   "source": [
    "## 인코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d446b1-e885-442b-8cec-bf96829fcdc2",
   "metadata": {},
   "source": [
    "### 정수 인코딩\n",
    "\n",
    "+ 단어 -> 정수화(index)\n",
    "    - good, hi, hello -> 0, 1, 2 -> 100 010 001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "975d72f1-b660-498b-ae40-d59511a1c578",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = '''Tokenization is a key (and mandatory) aspect of working with text data\n",
    "We’ll discuss the various nuances of tokenization, including how to handle Out-of-Vocabulary words (OOV)\n",
    "Language is a thing of beauty. But mastering a new language from scratch is quite a daunting prospect. If you’ve ever picked up a language that wasn’t your mother tongue, you’ll relate to this! There are so many layers to peel off and syntaxes to consider – it’s quite a challenge.\n",
    "And that’s exactly the way with our machines. In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand. That’s where the concept of tokenization in Natural Language Processing (NLP) comes in.\n",
    "Simply put, we can’t work with text data if we don’t perform tokenization. Yes, it’s really that important!'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fa41d70d-70b1-45c8-a517-f262dea3b76c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9,\n",
       " ['Tokenization is a key (and mandatory) aspect of working with text data\\nWe’ll discuss the various nuances of tokenization, including how to handle Out-of-Vocabulary words (OOV)\\nLanguage is a thing of beauty.',\n",
       "  'But mastering a new language from scratch is quite a daunting prospect.',\n",
       "  'If you’ve ever picked up a language that wasn’t your mother tongue, you’ll relate to this!',\n",
       "  'There are so many layers to peel off and syntaxes to consider – it’s quite a challenge.',\n",
       "  'And that’s exactly the way with our machines.',\n",
       "  'In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand.',\n",
       "  'That’s where the concept of tokenization in Natural Language Processing (NLP) comes in.',\n",
       "  'Simply put, we can’t work with text data if we don’t perform tokenization.',\n",
       "  'Yes, it’s really that important!'])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen = sent_tokenize(sentences)\n",
    "len(sen), sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1a23bd68-36c0-4d02-8b8a-082c44415594",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "pre_sents = []\n",
    "\n",
    "for s in sen:\n",
    "    s = s.lower()\n",
    "    wt = word_tokenize(s)\n",
    "    res = []\n",
    "    \n",
    "    for w in wt:\n",
    "        if w not in sw_eng:\n",
    "            if len(w) > 2:\n",
    "                res.append(w)\n",
    "                if w not in vocab:\n",
    "                    vocab[w] = 0\n",
    "                vocab[w] += 1\n",
    "    pre_sents.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f4f9af6e-b5bc-46a5-b6a1-7938b2673189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['tokenization',\n",
       "  'key',\n",
       "  'mandatory',\n",
       "  'aspect',\n",
       "  'working',\n",
       "  'text',\n",
       "  'data',\n",
       "  'discuss',\n",
       "  'various',\n",
       "  'nuances',\n",
       "  'tokenization',\n",
       "  'including',\n",
       "  'handle',\n",
       "  'out-of-vocabulary',\n",
       "  'words',\n",
       "  'oov',\n",
       "  'language',\n",
       "  'thing',\n",
       "  'beauty'],\n",
       " ['mastering', 'new', 'language', 'scratch', 'quite', 'daunting', 'prospect'],\n",
       " ['ever', 'picked', 'language', 'mother', 'tongue', 'relate'],\n",
       " ['many', 'layers', 'peel', 'syntaxes', 'consider', 'quite', 'challenge'],\n",
       " ['exactly', 'way', 'machines'],\n",
       " ['order',\n",
       "  'get',\n",
       "  'computer',\n",
       "  'understand',\n",
       "  'text',\n",
       "  'need',\n",
       "  'break',\n",
       "  'word',\n",
       "  'way',\n",
       "  'machine',\n",
       "  'understand'],\n",
       " ['concept',\n",
       "  'tokenization',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'processing',\n",
       "  'nlp',\n",
       "  'comes'],\n",
       " ['simply', 'put', 'work', 'text', 'data', 'perform', 'tokenization'],\n",
       " ['yes', 'really', 'important']]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d562e563-473d-47a1-b37c-e5e7ec7c6cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokenization': 4,\n",
       " 'key': 1,\n",
       " 'mandatory': 1,\n",
       " 'aspect': 1,\n",
       " 'working': 1,\n",
       " 'text': 3,\n",
       " 'data': 2,\n",
       " 'discuss': 1,\n",
       " 'various': 1,\n",
       " 'nuances': 1,\n",
       " 'including': 1,\n",
       " 'handle': 1,\n",
       " 'out-of-vocabulary': 1,\n",
       " 'words': 1,\n",
       " 'oov': 1,\n",
       " 'language': 4,\n",
       " 'thing': 1,\n",
       " 'beauty': 1,\n",
       " 'mastering': 1,\n",
       " 'new': 1,\n",
       " 'scratch': 1,\n",
       " 'quite': 2,\n",
       " 'daunting': 1,\n",
       " 'prospect': 1,\n",
       " 'ever': 1,\n",
       " 'picked': 1,\n",
       " 'mother': 1,\n",
       " 'tongue': 1,\n",
       " 'relate': 1,\n",
       " 'many': 1,\n",
       " 'layers': 1,\n",
       " 'peel': 1,\n",
       " 'syntaxes': 1,\n",
       " 'consider': 1,\n",
       " 'challenge': 1,\n",
       " 'exactly': 1,\n",
       " 'way': 2,\n",
       " 'machines': 1,\n",
       " 'order': 1,\n",
       " 'get': 1,\n",
       " 'computer': 1,\n",
       " 'understand': 2,\n",
       " 'need': 1,\n",
       " 'break': 1,\n",
       " 'word': 1,\n",
       " 'machine': 1,\n",
       " 'concept': 1,\n",
       " 'natural': 1,\n",
       " 'processing': 1,\n",
       " 'nlp': 1,\n",
       " 'comes': 1,\n",
       " 'simply': 1,\n",
       " 'put': 1,\n",
       " 'work': 1,\n",
       " 'perform': 1,\n",
       " 'yes': 1,\n",
       " 'really': 1,\n",
       " 'important': 1}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c06d533a-05f5-4cb4-aff2-0313a55c171d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1f1fc23a-04e2-41fc-b1a0-64bd5ae3326b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tokenization', 4),\n",
       " ('language', 4),\n",
       " ('text', 3),\n",
       " ('data', 2),\n",
       " ('quite', 2),\n",
       " ('way', 2),\n",
       " ('understand', 2),\n",
       " ('key', 1),\n",
       " ('mandatory', 1),\n",
       " ('aspect', 1),\n",
       " ('working', 1),\n",
       " ('discuss', 1),\n",
       " ('various', 1),\n",
       " ('nuances', 1),\n",
       " ('including', 1),\n",
       " ('handle', 1),\n",
       " ('out-of-vocabulary', 1),\n",
       " ('words', 1),\n",
       " ('oov', 1),\n",
       " ('thing', 1),\n",
       " ('beauty', 1),\n",
       " ('mastering', 1),\n",
       " ('new', 1),\n",
       " ('scratch', 1),\n",
       " ('daunting', 1),\n",
       " ('prospect', 1),\n",
       " ('ever', 1),\n",
       " ('picked', 1),\n",
       " ('mother', 1),\n",
       " ('tongue', 1),\n",
       " ('relate', 1),\n",
       " ('many', 1),\n",
       " ('layers', 1),\n",
       " ('peel', 1),\n",
       " ('syntaxes', 1),\n",
       " ('consider', 1),\n",
       " ('challenge', 1),\n",
       " ('exactly', 1),\n",
       " ('machines', 1),\n",
       " ('order', 1),\n",
       " ('get', 1),\n",
       " ('computer', 1),\n",
       " ('need', 1),\n",
       " ('break', 1),\n",
       " ('word', 1),\n",
       " ('machine', 1),\n",
       " ('concept', 1),\n",
       " ('natural', 1),\n",
       " ('processing', 1),\n",
       " ('nlp', 1),\n",
       " ('comes', 1),\n",
       " ('simply', 1),\n",
       " ('put', 1),\n",
       " ('work', 1),\n",
       " ('perform', 1),\n",
       " ('yes', 1),\n",
       " ('really', 1),\n",
       " ('important', 1)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(vocab.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cc20eeb4-1ec1-44b2-9be2-28b5dc645d04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokenization': 1,\n",
       " 'language': 2,\n",
       " 'text': 3,\n",
       " 'data': 4,\n",
       " 'quite': 5,\n",
       " 'way': 6,\n",
       " 'understand': 7}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = {}\n",
    "a = 0\n",
    "\n",
    "for k, v in sorted(vocab.items(), key=lambda x: x[1], reverse=True):\n",
    "    if v >= 2:\n",
    "        a += 1\n",
    "        word_index[k] = a\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "52ac69b2-b2ca-41dd-a642-0f6ff6badc9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tokenization is a key (and mandatory) aspect of working with text data\\nWe’ll discuss the various nuances of tokenization, including how to handle Out-of-Vocabulary words (OOV)\\nLanguage is a thing of beauty.',\n",
       " 'But mastering a new language from scratch is quite a daunting prospect.',\n",
       " 'If you’ve ever picked up a language that wasn’t your mother tongue, you’ll relate to this!',\n",
       " 'There are so many layers to peel off and syntaxes to consider – it’s quite a challenge.',\n",
       " 'And that’s exactly the way with our machines.',\n",
       " 'In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand.',\n",
       " 'That’s where the concept of tokenization in Natural Language Processing (NLP) comes in.',\n",
       " 'Simply put, we can’t work with text data if we don’t perform tokenization.',\n",
       " 'Yes, it’s really that important!']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "67ef9103-40d3-4433-a44a-fa2b4faf36b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index['OOV'] = len(word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "244ce65b-c414-4230-97ae-7d019f5f6a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sents = []\n",
    "for s in pre_sents:\n",
    "    enc_sent = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            enc_sent.append(word_index[w])\n",
    "        except KeyError:\n",
    "            enc_sent.append(word_index['OOV']) # OOV: Out Of Vocabulary: 코퍼스에 없는 단어라는 의미\n",
    "            \n",
    "    enc_sents.append(enc_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c20e0af6-a08c-4978-b538-2c48d274d07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 8, 8, 8, 8, 3, 4, 8, 8, 8, 1, 8, 8, 8, 8, 8, 2, 8, 8],\n",
       " [8, 8, 2, 8, 5, 8, 8],\n",
       " [8, 8, 2, 8, 8, 8],\n",
       " [8, 8, 8, 8, 8, 5, 8],\n",
       " [8, 6, 8],\n",
       " [8, 8, 8, 7, 3, 8, 8, 8, 6, 8, 7],\n",
       " [8, 1, 8, 2, 8, 8, 8],\n",
       " [8, 8, 8, 3, 4, 8, 1],\n",
       " [8, 8, 8]]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "06d62973-6dbb-49e1-a17a-0acd9bd091cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "37affc84-a72b-4b22-9ee1-5c4f43b84682",
   "metadata": {},
   "outputs": [],
   "source": [
    "tkt = Tokenizer()\n",
    "\n",
    "tkt.fit_on_texts(pre_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a4cc0ccc-3bda-49ba-8e3a-6d4bb233ca9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokenization': 1,\n",
       " 'language': 2,\n",
       " 'text': 3,\n",
       " 'data': 4,\n",
       " 'quite': 5,\n",
       " 'way': 6,\n",
       " 'understand': 7,\n",
       " 'key': 8,\n",
       " 'mandatory': 9,\n",
       " 'aspect': 10,\n",
       " 'working': 11,\n",
       " 'discuss': 12,\n",
       " 'various': 13,\n",
       " 'nuances': 14,\n",
       " 'including': 15,\n",
       " 'handle': 16,\n",
       " 'out-of-vocabulary': 17,\n",
       " 'words': 18,\n",
       " 'oov': 19,\n",
       " 'thing': 20,\n",
       " 'beauty': 21,\n",
       " 'mastering': 22,\n",
       " 'new': 23,\n",
       " 'scratch': 24,\n",
       " 'daunting': 25,\n",
       " 'prospect': 26,\n",
       " 'ever': 27,\n",
       " 'picked': 28,\n",
       " 'mother': 29,\n",
       " 'tongue': 30,\n",
       " 'relate': 31,\n",
       " 'many': 32,\n",
       " 'layers': 33,\n",
       " 'peel': 34,\n",
       " 'syntaxes': 35,\n",
       " 'consider': 36,\n",
       " 'challenge': 37,\n",
       " 'exactly': 38,\n",
       " 'machines': 39,\n",
       " 'order': 40,\n",
       " 'get': 41,\n",
       " 'computer': 42,\n",
       " 'need': 43,\n",
       " 'break': 44,\n",
       " 'word': 45,\n",
       " 'machine': 46,\n",
       " 'concept': 47,\n",
       " 'natural': 48,\n",
       " 'processing': 49,\n",
       " 'nlp': 50,\n",
       " 'comes': 51,\n",
       " 'simply': 52,\n",
       " 'put': 53,\n",
       " 'work': 54,\n",
       " 'perform': 55,\n",
       " 'yes': 56,\n",
       " 'really': 57,\n",
       " 'important': 58}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인코딩\n",
    "tkt.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8eee91ff-e028-493e-899a-81769d1080bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('tokenization', 4),\n",
       "             ('key', 1),\n",
       "             ('mandatory', 1),\n",
       "             ('aspect', 1),\n",
       "             ('working', 1),\n",
       "             ('text', 3),\n",
       "             ('data', 2),\n",
       "             ('discuss', 1),\n",
       "             ('various', 1),\n",
       "             ('nuances', 1),\n",
       "             ('including', 1),\n",
       "             ('handle', 1),\n",
       "             ('out-of-vocabulary', 1),\n",
       "             ('words', 1),\n",
       "             ('oov', 1),\n",
       "             ('language', 4),\n",
       "             ('thing', 1),\n",
       "             ('beauty', 1),\n",
       "             ('mastering', 1),\n",
       "             ('new', 1),\n",
       "             ('scratch', 1),\n",
       "             ('quite', 2),\n",
       "             ('daunting', 1),\n",
       "             ('prospect', 1),\n",
       "             ('ever', 1),\n",
       "             ('picked', 1),\n",
       "             ('mother', 1),\n",
       "             ('tongue', 1),\n",
       "             ('relate', 1),\n",
       "             ('many', 1),\n",
       "             ('layers', 1),\n",
       "             ('peel', 1),\n",
       "             ('syntaxes', 1),\n",
       "             ('consider', 1),\n",
       "             ('challenge', 1),\n",
       "             ('exactly', 1),\n",
       "             ('way', 2),\n",
       "             ('machines', 1),\n",
       "             ('order', 1),\n",
       "             ('get', 1),\n",
       "             ('computer', 1),\n",
       "             ('understand', 2),\n",
       "             ('need', 1),\n",
       "             ('break', 1),\n",
       "             ('word', 1),\n",
       "             ('machine', 1),\n",
       "             ('concept', 1),\n",
       "             ('natural', 1),\n",
       "             ('processing', 1),\n",
       "             ('nlp', 1),\n",
       "             ('comes', 1),\n",
       "             ('simply', 1),\n",
       "             ('put', 1),\n",
       "             ('work', 1),\n",
       "             ('perform', 1),\n",
       "             ('yes', 1),\n",
       "             ('really', 1),\n",
       "             ('important', 1)])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 카운팅\n",
    "tkt.word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7258ef35-bad4-43b9-9e6a-6d4424a66774",
   "metadata": {},
   "source": [
    "### 패딩\n",
    "\n",
    "+ 단어(PAD)에 대해 0으로 설정하여 길이를 맞춰줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "dc929bd1-3a49-4ba1-bc87-1ce0cb3320f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_sentences = [['driver', 'person'], ['driver', 'good', 'person'], ['driver', 'huge', 'person'], ['knew', 'bad'], ['bad', 'kept', 'huge', 'bad'], ['huge', 'bad']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "27587548-68a3-49ee-8cdb-141549c15809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 정수 인코딩\n",
    "tkt = Tokenizer()\n",
    "tkt.fit_on_texts(pre_sentences) # 모든 단어에 대해 번호를 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "aead7f64-50a4-4620-b8e9-61033bcf53e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3], [2, 5, 3], [2, 4, 3], [6, 1], [1, 7, 4, 1], [4, 1]]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tkt.texts_to_sequences(pre_sentences)\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c1abacad-8b1a-4a77-92b1-069e5044e40c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 길이 4로 통일 필요\n",
    "\n",
    "# 최대 길이 확인\n",
    "maxlen = max(len(i) for i in encoded)\n",
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "cd2dbfd1-f457-4d64-9a60-fdf229b0888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in encoded:\n",
    "    while len(s) < maxlen:\n",
    "        s.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "eddec0c5-8b25-4542-8600-23bb928aa7a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3, 0, 0],\n",
       " [2, 5, 3, 0],\n",
       " [2, 4, 3, 0],\n",
       " [6, 1, 0, 0],\n",
       " [1, 7, 4, 1],\n",
       " [4, 1, 0, 0]]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0bf6a0bb-17c0-4b23-8137-2709685789ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3, 0, 0],\n",
       "       [2, 5, 3, 0],\n",
       "       [2, 4, 3, 0],\n",
       "       [6, 1, 0, 0],\n",
       "       [1, 7, 4, 1],\n",
       "       [4, 1, 0, 0]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.array(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40f68e8-9754-4342-abd5-1ac3758ba15a",
   "metadata": {},
   "source": [
    "#### 케라스 도구를 이용한 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "398715ed-eabe-4061-9b3c-f1f593388303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9401f5f5-35a1-4811-a88d-0f46478a539b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3], [2, 5, 3], [2, 4, 3], [6, 1], [1, 7, 4, 1], [4, 1]]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tkt.texts_to_sequences(pre_sentences)\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "106027d6-91c3-4035-b8e3-fbaf4170459f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 2, 3],\n",
       "       [0, 2, 5, 3],\n",
       "       [0, 2, 4, 3],\n",
       "       [0, 0, 6, 1],\n",
       "       [1, 7, 4, 1],\n",
       "       [0, 0, 4, 1]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad_sequences(encoded)\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ddf78deb-50a5-4363-8097-79afec42d800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3, 0, 0],\n",
       "       [2, 5, 3, 0],\n",
       "       [2, 4, 3, 0],\n",
       "       [6, 1, 0, 0],\n",
       "       [1, 7, 4, 1],\n",
       "       [4, 1, 0, 0]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0 뒤로 보내기\n",
    "padded = pad_sequences(encoded, padding='post')\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "51c69824-2e96-4e5b-aa45-3a19125e3850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [2, 5, 3, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [2, 4, 3, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [6, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 7, 4, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [4, 1, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# maxlen\n",
    "padded = pad_sequences(encoded, padding='post', maxlen=10)\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7dde84a6-c263-456e-b7cb-f569ba3d7739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 3 0 0]\n",
      " [2 5 3 0]\n",
      " [2 4 3 0]\n",
      " [6 1 0 0]\n",
      " [1 7 4 1]\n",
      " [4 1 0 0]]\n",
      "[[2 3]\n",
      " [2 5]\n",
      " [2 4]\n",
      " [6 1]\n",
      " [1 7]\n",
      " [4 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2, 3],\n",
       "       [5, 3],\n",
       "       [4, 3],\n",
       "       [6, 1],\n",
       "       [4, 1],\n",
       "       [4, 1]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 만약 최대 길이보다 maxlen을 더 낮게 쓴다면?\n",
    "print(padded)\n",
    "\n",
    "padded = pad_sequences(encoded, padding='post', maxlen=2, truncating='post')\n",
    "print(padded)\n",
    "\n",
    "padded = pad_sequences(encoded, padding='post', maxlen=2, truncating='pre')\n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcba758b-cf03-4687-a494-586e5a241dcb",
   "metadata": {},
   "source": [
    "### 원핫인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7c7362d6-8d8e-4e82-9674-be34dfa0c0bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['자연어', '처리', '공부', '를', '합니다']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tks = okt.morphs('자연어처리 공부를 합니다')\n",
    "tks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "517483d5-c446-4b64-97fd-abdb9999345b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'소고기볶음밥': 1,\n",
       " '점심': 2,\n",
       " '메뉴로': 3,\n",
       " '먹었습니다': 4,\n",
       " '너무': 5,\n",
       " '맛있어요': 6,\n",
       " '또': 7,\n",
       " '먹을래요': 8}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '점심 메뉴로 소고기볶음밥 먹었습니다. 소고기볶음밥 너무 맛있어요. 또 먹을래요.'\n",
    "\n",
    "tkt = Tokenizer()\n",
    "tkt.fit_on_texts([text])\n",
    "\n",
    "tkt.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f623f888-61de-46dd-842c-e3f9df8a1db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 1, 7]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = '내일 메뉴로 소고기볶음밥 또 나왔으면 좋겠다.'\n",
    "\n",
    "encoded = tkt.texts_to_sequences([test])[0]\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "127cf401-e966-415c-a426-4a70e58f0e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "68175113-31d4-4ac2-b075-57345218a8e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_categorical(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28989954-8ee1-4fca-97a1-03e17f5add81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fef84e-501c-4c4d-8a1e-7302fe09e4c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f13db5d-b1ae-463c-ac44-8fec4bd06d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f8334f-70a7-45d8-a19c-685b440b7293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b1116b-a03b-43ee-9f05-959fd7171503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d954848-5aa8-4186-abcb-07f6cd3c960d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
