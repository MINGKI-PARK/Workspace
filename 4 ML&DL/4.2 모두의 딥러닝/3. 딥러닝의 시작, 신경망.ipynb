{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62fad43a",
   "metadata": {},
   "source": [
    "## 7. 퍼셉트론과 인공지능의 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a92b9c78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T04:36:51.766795Z",
     "start_time": "2023-07-13T04:36:51.368880Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78970cbb",
   "metadata": {},
   "source": [
    "### 7.1 인공지능의 시작을 알린 퍼셉트론\n",
    "\n",
    "+ 퍼셉트론(perceptron)\n",
    "+ 가중합(weighted sum)이란 입력 값과 가중치를 모두 곱한 후 바이어스를 더한 값을 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5be1dba",
   "metadata": {},
   "source": [
    "### 7.2 퍼셉트론의 과제\n",
    "\n",
    "+ 퍼셉트론이나 아달라인은 모두 2차원 평면상에 직선을 긋는 것만 가능. 경우에 따라 선을 아무리 그어도 해결되지 않는 상황 예시"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c531f2d",
   "metadata": {},
   "source": [
    "### 7.3 XOR 문제\n",
    "\n",
    "+ XOR(exclusive OR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1003c29a",
   "metadata": {},
   "source": [
    "## 8. 다층 퍼셉트론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df67d5d",
   "metadata": {},
   "source": [
    "### 8.1 다층 퍼셉트론의 등장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc251a17",
   "metadata": {},
   "source": [
    "### 8.2 다층 퍼셉트론의 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27681cde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T01:23:52.441943Z",
     "start_time": "2023-07-13T01:23:52.435960Z"
    }
   },
   "outputs": [],
   "source": [
    "w11 = np.array([-2, -2])\n",
    "w12 = np.array([2, 2])\n",
    "w2 = np.array([1, 1])\n",
    "\n",
    "b1, b2, b3 = 3, -1, -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "470c35a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T01:24:52.492334Z",
     "start_time": "2023-07-13T01:24:52.476751Z"
    }
   },
   "outputs": [],
   "source": [
    "def MLP(x, w, b):\n",
    "    y = np.sum(w * x) + b\n",
    "    \n",
    "    if y <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03282f70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T01:27:07.604732Z",
     "start_time": "2023-07-13T01:27:07.591770Z"
    }
   },
   "outputs": [],
   "source": [
    "# NAND 게이트\n",
    "def NAND(x1, x2):\n",
    "    return MLP(np.array([x1, x2]), w11, b1)\n",
    "\n",
    "# OR 게이트\n",
    "def OR(x1, x2):\n",
    "    return MLP(np.array([x1, x2]), w12, b2)\n",
    "\n",
    "# AND 게이트\n",
    "def AND(x1, x2):\n",
    "    return MLP(np.array([x1, x2]), w2, b3)\n",
    "\n",
    "# XOR 게이트\n",
    "def XOR(x1, x2):\n",
    "    return AND(NAND(x1, x2), OR(x1, x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f6c6f74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T01:28:47.098074Z",
     "start_time": "2023-07-13T01:28:47.086533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 값: (0, 0) // 출력값:0\n",
      "입력 값: (1, 0) // 출력값:1\n",
      "입력 값: (0, 1) // 출력값:1\n",
      "입력 값: (1, 1) // 출력값:0\n"
     ]
    }
   ],
   "source": [
    "for x in [(0, 0), (1, 0), (0, 1), (1, 1)]:\n",
    "    y = XOR(x[0], x[1])\n",
    "    print('입력 값: ' + str(x) +' // '+ '출력값:' + str(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9eefbd2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T01:34:21.354563Z",
     "start_time": "2023-07-13T01:34:21.332974Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 값: (0, 0) // 출력 값: 0\n",
      "입력 값: (1, 0) // 출력 값: 1\n",
      "입력 값: (0, 1) // 출력 값: 1\n",
      "입력 값: (1, 1) // 출력 값: 0\n"
     ]
    }
   ],
   "source": [
    "# 가중치와 바이어스\n",
    "w11 = np.array([-2, -2])\n",
    "w12 = np.array([2, 2])\n",
    "w2 = np.array([1, 1])\n",
    "\n",
    "b1, b2, b3 = 3, -1, -1\n",
    "\n",
    "# 퍼셉트론\n",
    "def MLP(x, w, b):\n",
    "    y = np.sum(w * x) + b\n",
    "    if y <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "# NAND 게이트\n",
    "def NAND(x1, x2):\n",
    "    return MLP(np.array([x1, x2]), w11, b1)\n",
    "\n",
    "# OR 게이트\n",
    "def OR(x1, x2):\n",
    "    return MLP(np.array([x1, x2]), w12, b2)\n",
    "\n",
    "# AND 게이트\n",
    "def AND(x1, x2):\n",
    "    return MLP(np.array([x1, x2]), w2, b3)\n",
    "\n",
    "# XOR 게이트\n",
    "def XOR(x1, x2):\n",
    "    return AND(NAND(x1, x2), OR(x1, x2))\n",
    "\n",
    "# x1 값, x2 값을 번갈아 대입하며 최종 값 출력\n",
    "for x in [(0, 0), (1, 0), (0, 1), (1, 1)]:\n",
    "    y = XOR(x[0], x[1])\n",
    "    print('입력 값: ' + str(x) + ' // ' + '출력 값: ' + str(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed13ed8",
   "metadata": {},
   "source": [
    "## 9. 오차 역전파에서 딥러닝으로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c58c6ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T04:04:49.067840Z",
     "start_time": "2023-07-13T04:04:49.039828Z"
    }
   },
   "outputs": [],
   "source": [
    "# 입력 값 및 타깃 값\n",
    "data = [\n",
    "    [[0, 0], [0]],\n",
    "    [[0, 1], [1]],\n",
    "    [[1, 0], [1]],\n",
    "    [[1, 1], [0]]\n",
    "]\n",
    "\n",
    "# 실행 횟수(iterations), 학습률(lr), 모멘텀 계수(mo) 설정\n",
    "iterations = 5000\n",
    "lr = .1\n",
    "mo = .9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0d21c32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T04:08:48.006735Z",
     "start_time": "2023-07-13T04:08:47.986583Z"
    }
   },
   "outputs": [],
   "source": [
    "# 활성화 함수 -> 1. 시그모이드\n",
    "# 미분할 때와 아닐 때 각각의 값\n",
    "def sigmoid(x, derivative=False):\n",
    "    if (derivative==True):\n",
    "        return x * (1 - x)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# 활성화 함수 -> 2. tanh\n",
    "# tanh 함수의 미분은 1 - (활성화 함수 출력의 제곱)\n",
    "def tanh(x, derivative=False):\n",
    "    if (derivative==True):\n",
    "        return 1 - x**2\n",
    "    return np.tanh(x)\n",
    "\n",
    "# 가중치 배열을 만드는 함수\n",
    "def makeMatrix(i, j, fill=0.0):\n",
    "    mat = []\n",
    "    for i in range(i):\n",
    "        mat.append([fill] * j)\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a57b4219",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T04:39:11.295175Z",
     "start_time": "2023-07-13T04:39:11.062465Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NeuralNetwork' object has no attribute 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 신경망의 실행\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mNeuralNetwork\u001b[39;00m:\n\u001b[0;32m      3\u001b[0m     \n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# 초깃값 지정\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_x, num_yh, num_yo, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      6\u001b[0m         \n\u001b[0;32m      7\u001b[0m         \u001b[38;5;66;03m# 입력 값(num_x), 은닉층의 초깃값(num_yh), 출력층의 초깃값(num_yo), 바이어스\u001b[39;00m\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_x \u001b[38;5;241m=\u001b[39m num_x \u001b[38;5;241m+\u001b[39m bias \u001b[38;5;66;03m# 바이어스는 1로 설정\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 130\u001b[0m, in \u001b[0;36mNeuralNetwork\u001b[1;34m()\u001b[0m\n\u001b[0;32m    127\u001b[0m n \u001b[38;5;241m=\u001b[39m NeuralNetwork(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# 학습 실행\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m \u001b[43mn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m(data)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# 결괏값 출력\u001b[39;00m\n\u001b[0;32m    133\u001b[0m n\u001b[38;5;241m.\u001b[39mresult(data)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NeuralNetwork' object has no attribute 'train'"
     ]
    }
   ],
   "source": [
    "# 신경망의 실행\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    # 초깃값 지정\n",
    "    def __init__(self, num_x, num_yh, num_yo, bias=1):\n",
    "        \n",
    "        # 입력 값(num_x), 은닉층의 초깃값(num_yh), 출력층의 초깃값(num_yo), 바이어스\n",
    "        self.num_x = num_x + bias # 바이어스는 1로 설정\n",
    "        self.num_yh = num_yh\n",
    "        self.num_yo = num_yo\n",
    "        \n",
    "        # 활성화 함수 초깃값\n",
    "        self.activation_input = [1.0] * self.num_x\n",
    "        self.activation_hidden = [1.0] * self.num_yh\n",
    "        self.activation_out = [1.0] * self.num_yo\n",
    "        \n",
    "        # 가중치 입력 초깃값\n",
    "        self.weight_in = makeMatrix(self.num_x, self.num_yh)\n",
    "        for i in range(self.num_x):\n",
    "            for j in range(self.num_yh):\n",
    "                self.weight_in[i][j] = random.random()\n",
    "                \n",
    "        # 가중치 출력 초깃값\n",
    "        self.weight_out = makeMatrix(self.num_yh, self.num_yo)\n",
    "        for j in range(self.num_yh):\n",
    "            for k in range(self.num_yo):\n",
    "                self.weight_out[j][k] = random.random()\n",
    "                \n",
    "        # 모멘텀 SGD를 위한 이전 가중치 초깃값\n",
    "        self.gradient_in = makeMatrix(self.num_x, self.num_yh)\n",
    "        self.gradient_out = makeMatrix(self.num_yh, self.num_yo)\n",
    "        \n",
    "    # 업데이트 함수\n",
    "    def update(self, inputs):\n",
    "\n",
    "        # 입력층의 활성화 함수\n",
    "        for i in range(self.num_x - 1):\n",
    "            self.activation_input[i] = inputs[i]\n",
    "\n",
    "        # 은닉층의 활성화 함수\n",
    "        for j in range(self.num_yh):\n",
    "            sum = 0.0\n",
    "            for i in range(self.num_x):\n",
    "                sum = sum + self.activation_input[i] * self.weight_in[i][j]\n",
    "\n",
    "            # 시그모이드와 tanh 중에서 활성화 함수 선택\n",
    "            self.activation_hidden[j] = tanh(sum, False)\n",
    "\n",
    "\n",
    "        # 출력층의 활성화 함수\n",
    "        for k in range(self.num_yo):\n",
    "            sum = 0.0\n",
    "            for j in range(self.num_yh):\n",
    "                sum = sum + self.activation_hidden[j] * self.weight_out[j][k]\n",
    "\n",
    "            # 시그모이드와 tanh 중에서 활성화 함수 선택\n",
    "            self.activation_out[k] = tanh(sum, False)\n",
    "\n",
    "        return self.activation_out[:]\n",
    "    \n",
    "    # 역전파 실행\n",
    "    def backPropagate(self, target):\n",
    "\n",
    "        # 델타 출력 계산\n",
    "        output_deltas = [0.0] * self.num_yo\n",
    "        for k in range(self.num_yo):\n",
    "            error = targets[k] - self.activation_out[k]\n",
    "\n",
    "            # 시그모이드와 tanh 중에서 활성화 함수 선택, 미분 적용\n",
    "            output_deltas[k] = tanh(self.activation_out[k], True) * error\n",
    "\n",
    "        # 은닉 노드와 오차 함수\n",
    "        hidden_deltas = [0.0] * self.num_yh\n",
    "        for j in range(self.num_yh):\n",
    "            error = 0.0\n",
    "            for k in range(self.num_yo):\n",
    "                error = error + output_deltas[k] * self.weight_out[j][k]\n",
    "            # 시그모이드와 tanh 중에서 활성화 함수 선택, 미분 적용\n",
    "            hidden_deltas[j] = tanh(self.activation_hidden[j], True) * error\n",
    "\n",
    "        # 출력 가중치 업데이트\n",
    "        for j in range(self.num_yh):\n",
    "            for k in range(self.num_yo):\n",
    "                gradient = output_deltas[k] * self.activation_hidden[j]\n",
    "                v = mo*self.gradient_out[j][k] - lr * gradient\n",
    "                self.weight_out[j][k] += v\n",
    "                self.gradient_out[j][k] = gradient\n",
    "\n",
    "        # 입력 가중치 업데이트\n",
    "        for i in range(self.num_x):\n",
    "            for j in range(self.num_yh):\n",
    "                gradient = hidden_deltas[j] * self.activation_input[i]\n",
    "                v = mo * self.gradient_in[i][j] - lr * gradient\n",
    "                self.weight_in[i][j] += v\n",
    "                self.gradient_in[i][j] = gradient\n",
    "\n",
    "        # 오차 계산(최소 제곱법)\n",
    "        error = 0.0\n",
    "        for k in range(len(targets)):\n",
    "            error = error + 0.5 * (targets[k] - self.activation_out[k])**2\n",
    "        return error\n",
    "    \n",
    "    # 학습 실행\n",
    "    def train(self, patterns):\n",
    "        for i in range(iterations):\n",
    "            error = 0.0\n",
    "            for p in patterns:\n",
    "                inputs = p[0]\n",
    "                targets = p[1]\n",
    "                self.update(inputs)\n",
    "                error = error + self.backPropagate(target)\n",
    "\n",
    "            if i % 500 == 0:\n",
    "                print('error: %-.5f' % error)\n",
    "\n",
    "\n",
    "    # 결괏값 출력\n",
    "    def result(self, patterns):\n",
    "        for p in patterns:\n",
    "            print('Input: %s, Predict: %s' % (p[0], self.update(p[0])))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        # 두 개의 입력 값, 두 개의 레이어, 하나의 출력 값을 갖도록 설정\n",
    "        n = NeuralNetwork(2, 2, 1)\n",
    "\n",
    "        # 학습 실행\n",
    "        n.train(data)\n",
    "\n",
    "        # 결괏값 출력\n",
    "        n.result(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cb0d488",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T04:20:12.864423Z",
     "start_time": "2023-07-13T04:20:12.841414Z"
    }
   },
   "outputs": [],
   "source": [
    "# 업데이트 함수\n",
    "def update(self, inputs):\n",
    "    \n",
    "    # 입력층의 활성화 함수\n",
    "    for i in range(self.num_x - 1):\n",
    "        self.activation_input[i] = inputs[i]\n",
    "        \n",
    "    # 은닉층의 활성화 함수\n",
    "    for j in range(self.num_yh):\n",
    "        sum = 0.0\n",
    "        for i in range(self.num_x):\n",
    "            sum = sum + self.activation_input[i] * self.weight_in[i][j]\n",
    "            \n",
    "        # 시그모이드와 tanh 중에서 활성화 함수 선택\n",
    "        self.activation_hidden[j] = tanh(sum, False)\n",
    "        \n",
    "        \n",
    "    # 출력층의 활성화 함수\n",
    "    for k in range(self.num_yo):\n",
    "        sum = 0.0\n",
    "        for j in range(self.num_yh):\n",
    "            sum = sum + self.activation_hidden[j] * self.weight_out[j][k]\n",
    "            \n",
    "        # 시그모이드와 tanh 중에서 활성화 함수 선택\n",
    "        self.activation_out[k] = tanh(sum, False)\n",
    "    \n",
    "    return self.activation_out[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5c3f426",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T04:31:52.257345Z",
     "start_time": "2023-07-13T04:31:52.240994Z"
    }
   },
   "outputs": [],
   "source": [
    "# 역전파 실행\n",
    "def backPropagate(self, target):\n",
    "    \n",
    "    # 델타 출력 계산\n",
    "    output_deltas = [0.0] * self.num_yo\n",
    "    for k in range(self.num_yo):\n",
    "        error = targets[k] - self.activation_out[k]\n",
    "        \n",
    "        # 시그모이드와 tanh 중에서 활성화 함수 선택, 미분 적용\n",
    "        output_deltas[k] = tanh(self.activation_out[k], True) * error\n",
    "        \n",
    "    # 은닉 노드와 오차 함수\n",
    "    hidden_deltas = [0.0] * self.num_yh\n",
    "    for j in range(self.num_yh):\n",
    "        error = 0.0\n",
    "        for k in range(self.num_yo):\n",
    "            error = error + output_deltas[k] * self.weight_out[j][k]\n",
    "        # 시그모이드와 tanh 중에서 활성화 함수 선택, 미분 적용\n",
    "        hidden_deltas[j] = tanh(self.activation_hidden[j], True) * error\n",
    "        \n",
    "    # 출력 가중치 업데이트\n",
    "    for j in range(self.num_yh):\n",
    "        for k in range(self.num_yo):\n",
    "            gradient = output_deltas[k] * self.activation_hidden[j]\n",
    "            v = mo*self.gradient_out[j][k] - lr * gradient\n",
    "            self.weight_out[j][k] += v\n",
    "            self.gradient_out[j][k] = gradient\n",
    "            \n",
    "    # 입력 가중치 업데이트\n",
    "    for i in range(self.num_x):\n",
    "        for j in range(self.num_yh):\n",
    "            gradient = hidden_deltas[j] * self.activation_input[i]\n",
    "            v = mo * self.gradient_in[i][j] - lr * gradient\n",
    "            self.weight_in[i][j] += v\n",
    "            self.gradient_in[i][j] = gradient\n",
    "            \n",
    "    # 오차 계산(최소 제곱법)\n",
    "    error = 0.0\n",
    "    for k in range(len(targets)):\n",
    "        error = error + 0.5 * (targets[k] - self.activation_out[k])**2\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddccaf5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T04:39:54.474058Z",
     "start_time": "2023-07-13T04:39:54.134468Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NeuralNetwork' object has no attribute 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 신경망의 실행\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mNeuralNetwork\u001b[39;00m:\n\u001b[0;32m      3\u001b[0m     \n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# 초깃값 지정\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_x, num_yh, num_yo, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      6\u001b[0m         \n\u001b[0;32m      7\u001b[0m         \u001b[38;5;66;03m# 입력 값(num_x), 은닉층의 초깃값(num_yh), 출력층의 초깃값(num_yo), 바이어스\u001b[39;00m\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_x \u001b[38;5;241m=\u001b[39m num_x \u001b[38;5;241m+\u001b[39m bias \u001b[38;5;66;03m# 바이어스는 1로 설정\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 130\u001b[0m, in \u001b[0;36mNeuralNetwork\u001b[1;34m()\u001b[0m\n\u001b[0;32m    127\u001b[0m n \u001b[38;5;241m=\u001b[39m NeuralNetwork(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# 학습 실행\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m \u001b[43mn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m(data)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# 결괏값 출력\u001b[39;00m\n\u001b[0;32m    133\u001b[0m n\u001b[38;5;241m.\u001b[39mresult(data)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NeuralNetwork' object has no attribute 'train'"
     ]
    }
   ],
   "source": [
    "# 신경망의 실행\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    # 초깃값 지정\n",
    "    def __init__(self, num_x, num_yh, num_yo, bias=1):\n",
    "        \n",
    "        # 입력 값(num_x), 은닉층의 초깃값(num_yh), 출력층의 초깃값(num_yo), 바이어스\n",
    "        self.num_x = num_x + bias # 바이어스는 1로 설정\n",
    "        self.num_yh = num_yh\n",
    "        self.num_yo = num_yo\n",
    "        \n",
    "        # 활성화 함수 초깃값\n",
    "        self.activation_input = [1.0] * self.num_x\n",
    "        self.activation_hidden = [1.0] * self.num_yh\n",
    "        self.activation_out = [1.0] * self.num_yo\n",
    "        \n",
    "        # 가중치 입력 초깃값\n",
    "        self.weight_in = makeMatrix(self.num_x, self.num_yh)\n",
    "        for i in range(self.num_x):\n",
    "            for j in range(self.num_yh):\n",
    "                self.weight_in[i][j] = random.random()\n",
    "                \n",
    "        # 가중치 출력 초깃값\n",
    "        self.weight_out = makeMatrix(self.num_yh, self.num_yo)\n",
    "        for j in range(self.num_yh):\n",
    "            for k in range(self.num_yo):\n",
    "                self.weight_out[j][k] = random.random()\n",
    "                \n",
    "        # 모멘텀 SGD를 위한 이전 가중치 초깃값\n",
    "        self.gradient_in = makeMatrix(self.num_x, self.num_yh)\n",
    "        self.gradient_out = makeMatrix(self.num_yh, self.num_yo)\n",
    "        \n",
    "    # 업데이트 함수\n",
    "    def update(self, inputs):\n",
    "\n",
    "        # 입력층의 활성화 함수\n",
    "        for i in range(self.num_x - 1):\n",
    "            self.activation_input[i] = inputs[i]\n",
    "\n",
    "        # 은닉층의 활성화 함수\n",
    "        for j in range(self.num_yh):\n",
    "            sum = 0.0\n",
    "            for i in range(self.num_x):\n",
    "                sum = sum + self.activation_input[i] * self.weight_in[i][j]\n",
    "\n",
    "            # 시그모이드와 tanh 중에서 활성화 함수 선택\n",
    "            self.activation_hidden[j] = tanh(sum, False)\n",
    "\n",
    "\n",
    "        # 출력층의 활성화 함수\n",
    "        for k in range(self.num_yo):\n",
    "            sum = 0.0\n",
    "            for j in range(self.num_yh):\n",
    "                sum = sum + self.activation_hidden[j] * self.weight_out[j][k]\n",
    "\n",
    "            # 시그모이드와 tanh 중에서 활성화 함수 선택\n",
    "            self.activation_out[k] = tanh(sum, False)\n",
    "\n",
    "        return self.activation_out[:]\n",
    "    \n",
    "    # 역전파 실행\n",
    "    def backPropagate(self, target):\n",
    "\n",
    "        # 델타 출력 계산\n",
    "        output_deltas = [0.0] * self.num_yo\n",
    "        for k in range(self.num_yo):\n",
    "            error = targets[k] - self.activation_out[k]\n",
    "\n",
    "            # 시그모이드와 tanh 중에서 활성화 함수 선택, 미분 적용\n",
    "            output_deltas[k] = tanh(self.activation_out[k], True) * error\n",
    "\n",
    "        # 은닉 노드와 오차 함수\n",
    "        hidden_deltas = [0.0] * self.num_yh\n",
    "        for j in range(self.num_yh):\n",
    "            error = 0.0\n",
    "            for k in range(self.num_yo):\n",
    "                error = error + output_deltas[k] * self.weight_out[j][k]\n",
    "            # 시그모이드와 tanh 중에서 활성화 함수 선택, 미분 적용\n",
    "            hidden_deltas[j] = tanh(self.activation_hidden[j], True) * error\n",
    "\n",
    "        # 출력 가중치 업데이트\n",
    "        for j in range(self.num_yh):\n",
    "            for k in range(self.num_yo):\n",
    "                gradient = output_deltas[k] * self.activation_hidden[j]\n",
    "                v = mo*self.gradient_out[j][k] - lr * gradient\n",
    "                self.weight_out[j][k] += v\n",
    "                self.gradient_out[j][k] = gradient\n",
    "\n",
    "        # 입력 가중치 업데이트\n",
    "        for i in range(self.num_x):\n",
    "            for j in range(self.num_yh):\n",
    "                gradient = hidden_deltas[j] * self.activation_input[i]\n",
    "                v = mo * self.gradient_in[i][j] - lr * gradient\n",
    "                self.weight_in[i][j] += v\n",
    "                self.gradient_in[i][j] = gradient\n",
    "\n",
    "        # 오차 계산(최소 제곱법)\n",
    "        error = 0.0\n",
    "        for k in range(len(targets)):\n",
    "            error = error + 0.5 * (targets[k] - self.activation_out[k])**2\n",
    "        return error\n",
    "    \n",
    "    # 학습 실행\n",
    "    def train(self, patterns):\n",
    "        for i in range(iterations):\n",
    "            error = 0.0\n",
    "            for p in patterns:\n",
    "                inputs = p[0]\n",
    "                targets = p[1]\n",
    "                self.update(inputs)\n",
    "                error = error + self.backPropagate(target)\n",
    "\n",
    "            if i % 500 == 0:\n",
    "                print('error: %-.5f' % error)\n",
    "\n",
    "\n",
    "    # 결괏값 출력\n",
    "    def result(self, patterns):\n",
    "        for p in patterns:\n",
    "            print('Input: %s, Predict: %s' % (p[0], self.update(p[0])))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        # 두 개의 입력 값, 두 개의 레이어, 하나의 출력 값을 갖도록 설정\n",
    "        n = NeuralNetwork(2, 2, 1)\n",
    "\n",
    "        # 학습 실행\n",
    "        n.train(data)\n",
    "\n",
    "        # 결괏값 출력\n",
    "        n.result(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48cc7edf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T04:40:35.457899Z",
     "start_time": "2023-07-13T04:40:35.446587Z"
    }
   },
   "outputs": [],
   "source": [
    "n = NeuralNetwork(2, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb20be8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T04:36:58.037001Z",
     "start_time": "2023-07-13T04:36:57.963484Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NeuralNetwork' object has no attribute 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m n \u001b[38;5;241m=\u001b[39m NeuralNetwork(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# 학습 실행\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[43mn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m(data)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# 결괏값 출력\u001b[39;00m\n\u001b[0;32m     31\u001b[0m n\u001b[38;5;241m.\u001b[39mresult(data)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NeuralNetwork' object has no attribute 'train'"
     ]
    }
   ],
   "source": [
    "# 학습 실행\n",
    "def train(self, patterns):\n",
    "    for i in range(iterations):\n",
    "        error = 0.0\n",
    "        for p in patterns:\n",
    "            inputs = p[0]\n",
    "            targets = p[1]\n",
    "            self.update(inputs)\n",
    "            error = error + self.backPropagate(target)\n",
    "        \n",
    "        if i % 500 == 0:\n",
    "            print('error: %-.5f' % error)\n",
    "            \n",
    "\n",
    "# 결괏값 출력\n",
    "def result(self, patterns):\n",
    "    for p in patterns:\n",
    "        print('Input: %s, Predict: %s' % (p[0], self.update(p[0])))\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    # 두 개의 입력 값, 두 개의 레이어, 하나의 출력 값을 갖도록 설정\n",
    "    n = NeuralNetwork(2, 2, 1)\n",
    "    \n",
    "    # 학습 실행\n",
    "    n.train(data)\n",
    "    \n",
    "    # 결괏값 출력\n",
    "    n.result(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78a9416c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T04:41:48.118255Z",
     "start_time": "2023-07-13T04:41:48.074517Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NeuralNetwork' object has no attribute 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NeuralNetwork' object has no attribute 'train'"
     ]
    }
   ],
   "source": [
    "zz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2181224",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T04:44:15.224032Z",
     "start_time": "2023-07-13T04:44:14.955088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 0.94250\n",
      "error 0.04287\n",
      "error 0.00348\n",
      "error 0.00164\n",
      "error 0.00106\n",
      "error 0.00078\n",
      "error 0.00092\n",
      "error 0.00053\n",
      "error 0.00044\n",
      "error 0.00038\n",
      "[0, 0] -> [0.03036939032113823]\n",
      "[0, 1] -> [0.9817636240847771]\n",
      "[1, 0] -> [0.9816259907635363]\n",
      "[1, 1] -> [-0.025585374843295334]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import string\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "# calculate a random number where:  a <= rand < b\n",
    "def rand(a, b):\n",
    "    return (b-a)*random.random() + a\n",
    "\n",
    "# Make a matrix (we could use NumPy to speed this up)\n",
    "def makeMatrix(I, J, fill=0.0):\n",
    "    m = []\n",
    "    for i in range(I):\n",
    "        m.append([fill]*J)\n",
    "    return m\n",
    "\n",
    "# our sigmoid function, tanh is a little nicer than the standard 1/(1+e^-x)\n",
    "def sigmoid(x):\n",
    "    return math.tanh(x)\n",
    "\n",
    "# derivative of our sigmoid function, in terms of the output (i.e. y)\n",
    "def dsigmoid(y):\n",
    "    return 1.0 - y**2\n",
    "\n",
    "class NN:\n",
    "    def __init__(self, ni, nh, no):\n",
    "        # number of input, hidden, and output nodes\n",
    "        self.ni = ni + 1 # +1 for bias node\n",
    "        self.nh = nh\n",
    "        self.no = no\n",
    "\n",
    "        # activations for nodes\n",
    "        self.ai = [1.0]*self.ni\n",
    "        self.ah = [1.0]*self.nh\n",
    "        self.ao = [1.0]*self.no\n",
    "        \n",
    "        # create weights\n",
    "        self.wi = makeMatrix(self.ni, self.nh)\n",
    "        self.wo = makeMatrix(self.nh, self.no)\n",
    "        # set them to random vaules\n",
    "        for i in range(self.ni):\n",
    "            for j in range(self.nh):\n",
    "                self.wi[i][j] = rand(-0.2, 0.2)\n",
    "        for j in range(self.nh):\n",
    "            for k in range(self.no):\n",
    "                self.wo[j][k] = rand(-2.0, 2.0)\n",
    "\n",
    "        # last change in weights for momentum   \n",
    "        self.ci = makeMatrix(self.ni, self.nh)\n",
    "        self.co = makeMatrix(self.nh, self.no)\n",
    "\n",
    "    def update(self, inputs):\n",
    "        if len(inputs) != self.ni-1:\n",
    "            raise ValueError('wrong number of inputs')\n",
    "\n",
    "        # input activations\n",
    "        for i in range(self.ni-1):\n",
    "            #self.ai[i] = sigmoid(inputs[i])\n",
    "            self.ai[i] = inputs[i]\n",
    "\n",
    "        # hidden activations\n",
    "        for j in range(self.nh):\n",
    "            sum = 0.0\n",
    "            for i in range(self.ni):\n",
    "                sum = sum + self.ai[i] * self.wi[i][j]\n",
    "            self.ah[j] = sigmoid(sum)\n",
    "\n",
    "        # output activations\n",
    "        for k in range(self.no):\n",
    "            sum = 0.0\n",
    "            for j in range(self.nh):\n",
    "                sum = sum + self.ah[j] * self.wo[j][k]\n",
    "            self.ao[k] = sigmoid(sum)\n",
    "\n",
    "        return self.ao[:]\n",
    "\n",
    "\n",
    "    def backPropagate(self, targets, N, M):\n",
    "        if len(targets) != self.no:\n",
    "            raise ValueError('wrong number of target values')\n",
    "\n",
    "        # calculate error terms for output\n",
    "        output_deltas = [0.0] * self.no\n",
    "        for k in range(self.no):\n",
    "            error = targets[k]-self.ao[k]\n",
    "            output_deltas[k] = dsigmoid(self.ao[k]) * error\n",
    "\n",
    "        # calculate error terms for hidden\n",
    "        hidden_deltas = [0.0] * self.nh\n",
    "        for j in range(self.nh):\n",
    "            error = 0.0\n",
    "            for k in range(self.no):\n",
    "                error = error + output_deltas[k]*self.wo[j][k]\n",
    "            hidden_deltas[j] = dsigmoid(self.ah[j]) * error\n",
    "\n",
    "        # update output weights\n",
    "        for j in range(self.nh):\n",
    "            for k in range(self.no):\n",
    "                change = output_deltas[k]*self.ah[j]\n",
    "                self.wo[j][k] = self.wo[j][k] + N*change + M*self.co[j][k]\n",
    "                self.co[j][k] = change\n",
    "                #print N*change, M*self.co[j][k]\n",
    "\n",
    "        # update input weights\n",
    "        for i in range(self.ni):\n",
    "            for j in range(self.nh):\n",
    "                change = hidden_deltas[j]*self.ai[i]\n",
    "                self.wi[i][j] = self.wi[i][j] + N*change + M*self.ci[i][j]\n",
    "                self.ci[i][j] = change\n",
    "\n",
    "        # calculate error\n",
    "        error = 0.0\n",
    "        for k in range(len(targets)):\n",
    "            error = error + 0.5*(targets[k]-self.ao[k])**2\n",
    "        return error\n",
    "\n",
    "\n",
    "    def test(self, patterns):\n",
    "        for p in patterns:\n",
    "            print(p[0], '->', self.update(p[0]))\n",
    "\n",
    "    def weights(self):\n",
    "        print('Input weights:')\n",
    "        for i in range(self.ni):\n",
    "            print(self.wi[i])\n",
    "        print()\n",
    "        print('Output weights:')\n",
    "        for j in range(self.nh):\n",
    "            print(self.wo[j])\n",
    "\n",
    "    def train(self, patterns, iterations=1000, N=0.5, M=0.1):\n",
    "        # N: learning rate\n",
    "        # M: momentum factor\n",
    "        for i in range(iterations):\n",
    "            error = 0.0\n",
    "            for p in patterns:\n",
    "                inputs = p[0]\n",
    "                targets = p[1]\n",
    "                self.update(inputs)\n",
    "                error = error + self.backPropagate(targets, N, M)\n",
    "            if i % 100 == 0:\n",
    "                print('error %-.5f' % error)\n",
    "\n",
    "\n",
    "def demo():\n",
    "    # Teach network XOR function\n",
    "    pat = [\n",
    "        [[0,0], [0]],\n",
    "        [[0,1], [1]],\n",
    "        [[1,0], [1]],\n",
    "        [[1,1], [0]]\n",
    "    ]\n",
    "\n",
    "    # create a network with two input, two hidden, and one output nodes\n",
    "    n = NN(2, 2, 1)\n",
    "    # train it with some patterns\n",
    "    n.train(pat)\n",
    "    # test it\n",
    "    n.test(pat)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ef2ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf6b6da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f022492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f8dacf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2153a6e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
